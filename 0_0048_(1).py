# -*- coding: utf-8 -*-
"""0_0048 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/146AR6ze8b-8I-pAwZ_4fWju4onSUhUhe
"""

### Import libraries

# Fix randomness and hide warnings
seed = 42

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['PYTHONHASHSEED'] = str(seed)
os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter(action='ignore', category=Warning)

import numpy as np
np.random.seed(seed)

import logging

import random
random.seed(seed)

# Import tensorflow
import tensorflow as tf
from tensorflow import keras as tfk
from tensorflow.keras import layers as tfkl
tf.autograph.set_verbosity(0)
tf.get_logger().setLevel(logging.ERROR)
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
tf.random.set_seed(seed)
tf.compat.v1.set_random_seed(seed)
print(tf.__version__)

import pandas as pd
import seaborn as sns
from datetime import datetime
import matplotlib.pyplot as plt
plt.rc('font', size=16)
from sklearn.preprocessing import MinMaxScaler

# Commented out IPython magic to ensure Python compatibility.
#Connection to Google Drive

from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive/My Drive/ANNDL

#Array che contiene le categorie di ogni time series. Dim 48000x1
categories = np.load("categories.npy")
#Array che contiene i valori delle time sesries. Dim 48000x2776
training_data = np.load("training_data.npy")
#Array che contiene gli indici che descrivono gli estremi degli intervalli validi delle time series. Dim 48000x2
valid_periods = np.load("valid_periods.npy")

categories.shape, training_data.shape, valid_periods.shape

# for sample in range(training_data.shape[0]):
#     if (valid_periods[sample,1] - valid_periods[sample,0]) < 210 and (valid_periods[sample,1] - valid_periods[sample,0] > 150):
#         period_len = valid_periods[sample,1] -  valid_periods[sample,0]
#         padding_len = 210 - period_len
#         training_data[sample, (valid_periods[sample,0] - padding_len) : valid_periods[sample,0]] = 0.0
#         valid_periods[sample,0] = valid_periods[sample,0] - padding_len

def clean_data(training_data, garbage_threshold = 210):
    cleaned_data = []
    cleaned_categories = []
    cleaned_valid_periods = []
    for i in range(training_data.shape[0]):
        if valid_periods[i][1] - valid_periods[i][0] >= garbage_threshold:
            cleaned_data.append(training_data[i])
            cleaned_categories.append(categories[i])
            cleaned_valid_periods.append(valid_periods[i])
    return np.array(cleaned_data), np.array(cleaned_categories), np.array(cleaned_valid_periods)

cleaned_data, cleaned_categories, cleaned_valid_periods = clean_data(training_data)

cleaned_categories.shape, cleaned_data.shape, cleaned_valid_periods.shape

from scipy.stats import iqr
my_iqr = iqr(cleaned_data, axis=1)
med = np.median(cleaned_data, axis=1)
mean_med = np.mean(med)
mean_iqr = np.mean(my_iqr)
mean_med, mean_iqr

cleaned_data = (cleaned_data - mean_med) / mean_iqr

x_min = np.min(cleaned_data,axis=1)
x_max = np.max(cleaned_data,axis=1)
np.mean(x_min), np.mean(x_max)

cleaned_data = (cleaned_data - np.mean(x_min)) / (np.mean(x_max) - np.mean(x_min))

plt.plot(cleaned_data[10])
plt.plot(training_data[10])

def split_for_categories(training_data, categories):
    all_samples_list = []
    for i in np.unique(categories):
        mask = np.where(categories == i, True, False)

        samples_for_category = training_data[mask]
        all_samples_list.append(samples_for_category)
    return np.array(all_samples_list[0]), np.array(all_samples_list[1]), np.array(all_samples_list[2]), np.array(all_samples_list[3]), np.array(all_samples_list[4]), np.array(all_samples_list[5])

data_A, data_B, data_C, data_D, data_E, data_F = split_for_categories(cleaned_data, cleaned_categories)

data_A.shape, data_B.shape, data_C.shape, data_D.shape, data_E.shape, data_F.shape

training_set = np.vstack((data_A, data_B, data_C, data_D, data_E, data_F))
#test_set = np.vstack((test_set_A,test_set_B,test_set_C,test_set_D,test_set_E
                         #,test_set_F))
np.random.shuffle(training_set)
#np.random.shuffle(test_set)
training_set.shape #, test_set.shape

def build_sequences(df, window=200, stride=5, telescope=18):

    # Sanity check to avoid runtime errors
    assert window % stride == 0
    dataset = []
    labels = []
    temp_df = df.copy()
    padding_check = df.size%window

    #print(temp_df.size)

    if(padding_check != 0):
        # Compute padding length
        padding_len = window - df.size%window
        padding = np.zeros((padding_len), dtype='float32')
        temp_df = np.concatenate((padding,df))
        assert temp_df.size % window == 0

    #print(temp_df.size)
    for idx in np.arange(0,temp_df.size-window-telescope,stride):
        dataset.append(temp_df[idx:idx+window])
        labels.append(temp_df[idx+window:idx+window+telescope])

    return np.vstack(dataset), np.vstack(labels)

#data is a pandas series containing 48000 lists (either training, validation or test)
def THE_SEQUENCER(data):
    dataset = []
    labels = []
    for i in range(data.shape[0]):
        time_series = data[i][cleaned_valid_periods[i][0]:cleaned_valid_periods[i][1]]
        dset, labs = build_sequences(time_series)
        if len(dset) == 0:
            continue
        dataset.append(dset)
        labels.append(labs)

    return dataset, labels

train_sequences, train_labels = THE_SEQUENCER(training_set)
#test_sequences, test_labels = THE_SEQUENCER(test_set)

def residual_block(x, filters, kernel_size=3, strides=1):
    # Shortcut
    shortcut = x

    # First convolution
    x = tfkl.Conv1D(filters, kernel_size, strides=strides, padding='same')(x)
    x = tfkl.BatchNormalization()(x)
    x = tfkl.Activation('relu')(x)

    # Second convolution
    x = tfkl.Conv1D(filters, kernel_size, strides=1, padding='same')(x)
    x = tfkl.BatchNormalization()(x)

    # Add shortcut to the main path
    x = tfkl.Add()([x, shortcut])
    x = tfkl.Activation('relu')(x)

    return x

def build_resnet(input_shape, output_shape):
    input_layer = tfkl.Input(shape=input_shape)
    x = tfkl.Reshape((input_shape[0], 1), input_shape = input_shape)(input_layer) # (BS, 200, 1)

    # Initial convolution
    x = tfkl.Conv1D(64, 7, strides=2, padding='same')(x)
    x = tfkl.BatchNormalization()(x)
    x = tfkl.Activation('relu')(x)

    # Residual blocks
    for _ in range(3):
        x = residual_block(x, 64)

    x = tfkl.Attention(use_scale=True)([x, x])
    x = tfkl.LSTM(64, return_sequences=True)(x)

    x = tfkl.Conv1D(128, 3, strides=2, padding='same')(x)
    for _ in range(3):
        x = residual_block(x, 128)

    x = tfkl.Attention(use_scale=True)([x, x])
    x = tfkl.LSTM(128, return_sequences=True)(x)

    x = tfkl.Conv1D(256, 3, strides=2, padding='same')(x)
    for _ in range(3):
        x = residual_block(x, 256)

    # Global average pooling
    x = tfkl.GlobalAveragePooling1D()(x)

    # Fully connected layer
    x = tfkl.Dense(output_shape[0])(x)

    model = tfk.Model(inputs=input_layer, outputs=x, name='resnet')
    model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tfk.optimizers.Adam())
    #model.compile(loss=tf.keras.losses.MeanAbsoluteError(), optimizer=tfk.optimizers.Adam())

    return model

input_shape = (200,1)
output_shape = (18,1)
model = build_resnet(input_shape, output_shape)
model.summary()

X_train = []
y_train = []
for i in range(len(train_sequences)):
    for j in range(len(train_sequences[i])):
        X_train.append(train_sequences[i][j])
        y_train.append(train_labels[i][j])
X_train = np.array(X_train)
y_train = np.array(y_train)

# Train the model

batch_size = 128

history = model.fit(
  x = X_train,
  y = y_train,
  batch_size = batch_size,
  epochs = 200,
  validation_split=.1,
  callbacks = [
      tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=12, restore_best_weights=True, min_delta=0.0001),
      tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=8, factor=0.1, min_lr=1e-5)
  ]
).history

model.save('second_trial_tel18')

